@inproceedings{sarkar2024llmscannotexplain,
author = {Advait Sarkar},
title = {Large Language Models Cannot Explain Themselves},
year = {2024},
url = {https://arxiv.org/abs/2405.04382},
doi = {10.48550/arXiv.2405.04382},
abstract = {Large language models can be prompted to produce text. They can also be prompted to produce "explanations" of their output. But these are not really explanations, because they do not accurately reflect the mechanical process underlying the prediction. The illusion that they reflect the reasoning process can result in significant harms. These "explanations" can be valuable, but for promoting critical thinking rather than for understanding the model. I propose a recontextualisation of these "explanations", using the term "exoplanations" to draw attention to their exogenous nature. I discuss some implications for design and technology, such as the inclusion of appropriate guardrails and responses when models are prompted to generate explanations.},
booktitle = {Proceedings of the ACM CHI 2024 Workshop on Human-Centered Explainable AI},
keywords = {exoplanations, mechanismal explanations, co-audit, AI safety, explainable AI, XAI, critical thinking},
location = {Honolulu, HI, USA},
series = {HCXAI at CHI '24}
}