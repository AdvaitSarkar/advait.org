@inproceedings{barke2024solving,
    title = "Solving Data-centric Tasks using Large Language Models",
    author = "Barke, Shraddha  and
      Poelitz, Christian  and
      Negreanu, Carina  and
      Zorn, Benjamin  and
      Cambronero, Jos{\'e}  and
      Gordon, Andrew  and
      Le, Vu  and
      Nouri, Elnaz  and
      Polikarpova, Nadia  and
      Sarkar, Advait  and
      Slininger, Brian  and
      Toronto, Neil  and
      Williams, Jack",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-naacl.41",
    pages = "626--638",
    abstract = "Large language models are rapidly replacing help forums like StackOverflow, and are especially helpful to non-professional programmers and end users. These users are often interested in \textit{data-centric tasks}, like spreadsheet manipulation and data wrangling, which are hard to solve if the intent is only communicated using a natural-language description, without including data. But how do we decide how much data and which data to include in the prompt?This paper makes two contributions towards answering this question. First, we create a dataset of real-world NL-to-code tasks manipulating tabular data, mined from StackOverflow posts. Second, we introduce a novel \textit{cluster-then-select} prompting technique, which adds the most representative rows from the input data to the LLM prompt. Our experiments show that LLM performance is indeed sensitive to the amount of data passed in the prompt, and that for tasks with a lot of syntactic variation in the input table,our cluster-then-select technique outperforms a random selection baseline.",
}
